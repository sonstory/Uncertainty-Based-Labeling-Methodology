{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d2ade2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torchvision\n",
    "import os\n",
    "import natsort\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01fea80",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b7a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpu_setting:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125346d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "def metric_batch(output, target):\n",
    "    pred = output.argmax(1, keepdim=True)\n",
    "    corrects = pred.eq(target.view_as(pred)).sum().item()\n",
    "    return corrects\n",
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "    loss = loss_func(output, target)\n",
    "    metric_b = metric_batch(output, target)\n",
    "\n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    return loss.item(), metric_b\n",
    "\n",
    "def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):\n",
    "    running_loss = 0.0\n",
    "    running_metric = 0.0\n",
    "    len_data = len(dataset_dl.dataset)\n",
    "\n",
    "    for xb, yb in dataset_dl:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        output = model(xb)\n",
    "\n",
    "        loss_b, metric_b = loss_batch(loss_func, output, yb, opt)\n",
    "\n",
    "        running_loss += loss_b\n",
    "        \n",
    "        if metric_b is not None:\n",
    "            running_metric += metric_b\n",
    "        \n",
    "        if sanity_check is True:\n",
    "            break\n",
    "\n",
    "    loss = running_loss / len_data\n",
    "    metric = running_metric / len_data\n",
    "\n",
    "    return loss, metric\n",
    "\n",
    "def train_val(model, params, epoch):\n",
    "    num_epochs=epoch\n",
    "    loss_func=params[\"loss_func\"]\n",
    "    opt=params[\"optimizer\"]\n",
    "    train_dl=params[\"train_dl\"]\n",
    "    val_dl=params[\"val_dl\"]\n",
    "    sanity_check=params[\"sanity_check\"]\n",
    "    lr_scheduler=params[\"lr_scheduler\"]\n",
    "    path2weights=params[\"path2weights\"]\n",
    "\n",
    "    loss_history = {'train': [], 'val': []}\n",
    "    metric_history = {'train': [], 'val': []}\n",
    "\n",
    "    # # GPU out of memoty error\n",
    "    # best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr={}'.format(epoch, num_epochs-1, current_lr))\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)\n",
    "        loss_history['train'].append(train_loss)\n",
    "        metric_history['train'].append(train_metric)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)\n",
    "        loss_history['val'].append(val_loss)\n",
    "        metric_history['val'].append(val_metric)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            # best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            torch.save(model.state_dict(), path2weights)\n",
    "            print('Copied best model weights!')\n",
    "            print('Get best val_loss')\n",
    "\n",
    "        lr_scheduler.step(val_loss)\n",
    "\n",
    "        print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))\n",
    "        print('-'*10)\n",
    "\n",
    "    # model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d5f8d0",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25e753fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion),\n",
    "        )\n",
    "\n",
    "        # identity mapping, input과 output의 feature map size, filter 수가 동일한 경우 사용.\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # projection mapping using 1x1conv\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_block, num_classes=9, init_weights=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels=64\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        # weights inittialization\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        x = self.conv3_x(output)\n",
    "        x = self.conv4_x(x)\n",
    "        x = self.conv5_x(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    # define weight initialization function\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def resnet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def resnet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "def resnet50():\n",
    "    return ResNet(BottleNeck, [3,4,6,3])\n",
    "\n",
    "def resnet101():\n",
    "    return ResNet(BottleNeck, [3, 4, 23, 3])\n",
    "\n",
    "def resnet152():\n",
    "    return ResNet(BottleNeck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ce3f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNetParameters(model, train_dl, valid_dl):\n",
    "    device = gpu_setting.device\n",
    "    model = model.lower()\n",
    "    if model == 'resnet34':\n",
    "        model = resnet34().to(device)\n",
    "    if model == 'resnet50':\n",
    "        model = resnet50().to(device)\n",
    "    if model == 'resnet101':\n",
    "        model = resnet101().to(device)\n",
    "        \n",
    "    loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "    opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "    lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=10)\n",
    "    \n",
    "    # definc the training parameters\n",
    "    params_train = {\n",
    "        'num_epochs':21,\n",
    "        'optimizer':opt,\n",
    "        'loss_func':loss_func,\n",
    "        'train_dl':train_dl, \n",
    "        'val_dl':valid_dl,\n",
    "        'sanity_check':False,\n",
    "        'lr_scheduler':lr_scheduler,\n",
    "        'path2weights':'./model_weights/auto_labeling_res.pt', #이거 변경해서 사용\n",
    "    }\n",
    "    return model, params_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab5cee9",
   "metadata": {},
   "source": [
    "# Initial Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb79e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSubset(Dataset):\n",
    "    def __init__(self,Subset,transform=None):\n",
    "        super(CustomSubset,self).__init__()\n",
    "        self.Subset=Subset\n",
    "        self.indices=Subset.indices\n",
    "        self.transform=transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Subset)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        img,label=self.Subset[idx]\n",
    "        if self.transform is not None:\n",
    "            img=self.transform(img)\n",
    "        return img,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9a4dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data\n",
    "init_data_dir = '../../Reject_Option/Data/Labeled/'\n",
    "init_data_folder_dataset = dset.ImageFolder(root=init_data_dir)\n",
    "\n",
    "# train & validation\n",
    "train_data_len = int(len(init_data_folder_dataset)*0.8)\n",
    "valid_data_len = len(init_data_folder_dataset) - train_data_len\n",
    "train_data, valid_data = random_split(init_data_folder_dataset, [train_data_len, valid_data_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3850dc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13836 3459\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e215b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_data_transformation = transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor()])\n",
    "\n",
    "train_data = CustomSubset(train_data, init_data_transformation)\n",
    "valid_data = CustomSubset(valid_data, init_data_transformation)\n",
    "\n",
    "init_train_dl = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "init_valid_dl = DataLoader(valid_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775dc9e4",
   "metadata": {},
   "source": [
    "# Initial Model Train\n",
    "- 초기 데이터 17,295개에 대한 ResNet50 모델 학습 진행\n",
    "- epoch 5로 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d770898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d3748d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, params_train = ResNetParameters('resnet50', init_train_dl, init_valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "150fef75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4, current lr=0.001\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "train loss: 0.370385, val loss: 0.270913, accuracy: 92.74, time: 0.6030 min\n",
      "----------\n",
      "Epoch 1/4, current lr=0.001\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "train loss: 0.242176, val loss: 0.207427, accuracy: 93.67, time: 1.1591 min\n",
      "----------\n",
      "Epoch 2/4, current lr=0.001\n",
      "train loss: 0.207839, val loss: 0.474626, accuracy: 85.00, time: 1.7141 min\n",
      "----------\n",
      "Epoch 3/4, current lr=0.001\n",
      "train loss: 0.183102, val loss: 0.223500, accuracy: 93.50, time: 2.2665 min\n",
      "----------\n",
      "Epoch 4/4, current lr=0.001\n",
      "Copied best model weights!\n",
      "Get best val_loss\n",
      "train loss: 0.167128, val loss: 0.206422, accuracy: 94.45, time: 2.8181 min\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "model, loss_hist, metric_hist = train_val(model, params_train, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2727be6",
   "metadata": {},
   "source": [
    "# Test Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81474217",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet():\n",
    "    def __init__(self, main_dir, transform):\n",
    "        self.main_dir = main_dir\n",
    "        self.transform = transform\n",
    "        all_imgs = os.listdir(main_dir)\n",
    "        self.total_imgs = all_imgs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n",
    "        image = Image.open(img_loc).convert(\"RGB\")\n",
    "        tensor_image = self.transform(image)\n",
    "        return tensor_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc9f3609",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = '../../Reject_Option/Data/Unlabeled/'\n",
    "test_dataset = CustomDataSet(test_data_dir, transform=init_data_transformation)\n",
    "test_dl = DataLoader(test_dataset, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a08fde5",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a4e3316",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list =[]\n",
    "size = len(test_dl)\n",
    "model.eval()\n",
    "test_loss, correct = 0, 0\n",
    "with torch.no_grad():\n",
    "    for X in test_dl:\n",
    "        X = X.to(device)\n",
    "        pred = model(X)\n",
    "        sft = torch.nn.functional.softmax(pred, dim=1)\n",
    "        pred_list.append(sft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a629155a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155655"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9cffbf",
   "metadata": {},
   "source": [
    "# Labeling Accuracy\n",
    "17,295개의 초기 데이터로 학습된 모델로 155,655개의 unlabeled 데이터를 그냥 예측했을 때의 라벨링 정확도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "528434e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "label = pd.read_csv('../../../csv/data_155655.csv', index_col=0)\n",
    "\n",
    "all_imgs = os.listdir(test_data_dir)\n",
    "for i in range(0,155655):\n",
    "    label_list.append(int(pred_list[i][0].argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca909799",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_label_list = list(label['failureNum'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4cb948f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 155655/155655 [00:00<00:00, 3855301.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33839"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_cnt = 0\n",
    "for i in tqdm(range(0,155655)):\n",
    "    if label_list[i] != real_label_list[i]:\n",
    "        wrong_cnt = wrong_cnt + 1\n",
    "        \n",
    "wrong_cnt100%|█████████████████████████████████████████████████████████████████████| 155655/155655 [00:00<00:00, 3855301.04it/s]\n",
    "33839"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
